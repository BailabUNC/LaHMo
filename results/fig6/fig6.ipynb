{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, sosfiltfilt, find_peaks\n",
    "\n",
    "def break_correct(data):\n",
    "    corrected_data = np.copy(data)\n",
    "    for i in range(1, len(data)):\n",
    "        diff = data[i] - data[i - 1]\n",
    "        if diff < -180:  # Assuming a break from 360 to 0\n",
    "            corrected_data[i:] += 360\n",
    "        elif diff > 180:  # Assuming a break from 0 to 360\n",
    "            corrected_data[i:] -= 360\n",
    "    return corrected_data\n",
    "\n",
    "def signal_filter(data, fc1, fc2, fs, order=3):\n",
    "    sos = butter(order, fc1, btype='high', fs=fs, analog=False, output='sos')\n",
    "    filtered_data = sosfiltfilt(sos, data)\n",
    "    sos = butter(order, fc2, btype='low', fs=fs, analog=False, output='sos')\n",
    "    filtered_data = sosfiltfilt(sos, filtered_data)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../../dataset/basketball/test-2024-01-10-16-01-28.csv'\n",
    "data = pd.read_csv(file_path).to_numpy()\n",
    "timestamps = (data[:, 0] - data[0, 0])/ 1000\n",
    "timestamps = np.linspace(0, timestamps[-1], len(timestamps))\n",
    "c = data[:, 1]\n",
    "b = data[:, 2]\n",
    "tl = data[:, 3]\n",
    "tr = data[:, 4]\n",
    "r = data[:, 5]\n",
    "p = data[:, 6]\n",
    "y = data[:, 7]\n",
    "\n",
    "y = break_correct(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, c = upsample(timestamps, c, 100)\n",
    "_, b = upsample(timestamps, b, 100)\n",
    "_, tl = upsample(timestamps, tl, 100)\n",
    "_, tr = upsample(timestamps, tr, 100)\n",
    "_, y = upsample(timestamps, y, 100)\n",
    "_, p = upsample(timestamps, p, 100)\n",
    "_, r = upsample(timestamps, r, 100)\n",
    "timestamps, _ = upsample(timestamps, timestamps, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = signal_filter(c, 0.4, 8, 1000)\n",
    "b = signal_filter(b, 0.4, 8, 1000)\n",
    "tl = signal_filter(tl, 0.4, 8, 1000)\n",
    "tr = signal_filter(tr, 0.4, 8, 1000)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(timestamps, c)\n",
    "plt.plot(timestamps, b)\n",
    "plt.plot(timestamps, tl)\n",
    "plt.plot(timestamps, tr)\n",
    "\n",
    "# plt.xlim(480, 510)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highpass filter to remove drift (assuming sampling rate of 100 Hz, and cutoff frequency of 0.1 Hz)\n",
    "fs = 1000\n",
    "fc1 = 0.2\n",
    "fc2 = 50\n",
    "y = signal_filter(y, fc1, fc2, fs)\n",
    "p = signal_filter(p, fc1, fc2, fs)\n",
    "r = signal_filter(r, fc1, fc2, fs)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "# plt.plot(timestamps, y, 'k--', lw=0.5)\n",
    "# plt.plot(timestamps, p, 'b--', lw=0.5)\n",
    "# plt.plot(timestamps, r, 'g--', lw=0.5)\n",
    "plt.plot(timestamps, y, 'r', lw=0.5)\n",
    "plt.plot(timestamps, p, 'b', lw=0.5)\n",
    "plt.plot(timestamps, r, 'g', lw=0.5)\n",
    "\n",
    "\n",
    "# plt.xlim(264, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "events = pd.read_csv('../videos/basketball/data_points.csv', header=None).to_numpy().squeeze()\n",
    "plt.plot(timestamps, c)\n",
    "plt.plot(timestamps, b)\n",
    "for event in events:\n",
    "    plt.plot(event, c[np.where(np.abs(timestamps - event) <= 8e-2)[0][0]], 'rx')\n",
    "\n",
    "plt.xlim(560, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, _ = find_peaks(b, distance=1000, height=0)\n",
    "\n",
    "modified_events = []\n",
    "\n",
    "for event in events:\n",
    "    differences = np.abs(peaks - event * 1000)\n",
    "    min_diff_index = np.argmin(differences)\n",
    "    if differences[min_diff_index] <= 1000:\n",
    "        modified_events.append(peaks[min_diff_index])\n",
    "\n",
    "modified_events = np.array(modified_events)\n",
    "plt.plot(timestamps, b)\n",
    "plt.plot(timestamps[peaks], b[peaks], 'x')\n",
    "plt.xlim(600, 620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timestamps, c)\n",
    "plt.plot(timestamps, b)\n",
    "for event in modified_events:\n",
    "    plt.plot(timestamps[event], b[event], 'rx')\n",
    "\n",
    "plt.xlim(600, 620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.column_stack((c, b, tl, tr, y, p, r))\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time period for each activity, [start_idx, end_idx]\n",
    "jogging = [580000, 670000]\n",
    "resting = [670000, 780000]\n",
    "shooting = [780000, 1184000]\n",
    "layup = [1184000, 1560000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps1 = timestamps * 0.001 / (timestamps[1] - timestamps[0])\n",
    "events1 = modified_events * 0.001 / (timestamps[1] - timestamps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_find_peaks(activity, signal, height=None, distance=None):\n",
    "    section_signal = signal[activity[0]:activity[1]]\n",
    "    section_timestamps = timestamps1[activity[0]:activity[1]]\n",
    "    idx, _ = find_peaks(section_signal, height=height, distance=distance)\n",
    "    plt.plot(section_timestamps, section_signal)\n",
    "    plt.plot(section_timestamps[idx], section_signal[idx], marker=\"o\", ls=\"\", ms=3)\n",
    "    return section_timestamps[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jogging_peak_time, jogging_peak = manual_find_peaks(jogging, c, height=0.05, distance=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resting_peak_time, resting_peak = manual_find_peaks(resting, b, height=0.2, distance=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_peak_time, shooting_peak = manual_find_peaks(shooting, c, height=0.15, distance=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layup_peak_time, layup_peak = manual_find_peaks(layup, c, height=0.15, distance=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 3000\n",
    "\n",
    "num_events = len(modified_events)\n",
    "temp_container = np.zeros((num_events, window_width))\n",
    "\n",
    "for idx, event in enumerate(modified_events):\n",
    "    start = event - window_width // 2\n",
    "    end = event + window_width // 2\n",
    "    temp_container[idx, :] = b[start:end]\n",
    "\n",
    "avg = np.mean(temp_container, axis=0)\n",
    "std = np.std(temp_container, axis=0)\n",
    "\n",
    "mask = np.ones(len(temp_container), dtype=bool)\n",
    "\n",
    "for idx, sample in enumerate(temp_container):\n",
    "    if np.any(sample > avg + 2*std) or np.any(sample < avg - 2*std):\n",
    "        mask[idx] = False\n",
    "\n",
    "modified_events = modified_events[mask]\n",
    "cough_peak_time = (modified_events / 1000).squeeze()\n",
    "print(cough_peak_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_time = np.concatenate([cough_peak_time,\n",
    "                            jogging_peak_time,\n",
    "                            resting_peak_time,\n",
    "                            shooting_peak_time,\n",
    "                            layup_peak_time])[:, np.newaxis]\n",
    "peak_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cough 1, resting 0, jogging/shooting/layup 2\n",
    "peak_activities = np.array(([1] * cough_peak_time.size) + \\\n",
    "                           ([2] * jogging_peak_time.size) + \\\n",
    "                           ([0] * resting_peak_time.size) + \\\n",
    "                           ([2] * shooting_peak_time.size) + \\\n",
    "                           ([2] * layup_peak_time.size))[:, np.newaxis]\n",
    "peak_activities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_time_activities = np.concatenate([peak_time, peak_activities], axis=1)\n",
    "peak_time_activities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_time_activities[peak_time_activities[:, 1] == 1][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'arial'\n",
    "mpl.rcParams['font.size'] = 14\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot()\n",
    "plt.plot(timestamps1, b, c='firebrick')\n",
    "plt.plot(peak_time_activities[peak_time_activities[:, 1] == 1][:, 0],\n",
    "         2 * np.ones_like(peak_time_activities[peak_time_activities[:, 1] == 1][:, 0]),\n",
    "         'v', ms=2)\n",
    "plt.plot(peak_time_activities[peak_time_activities[:, 1] != 1][:, 0],\n",
    "         2.1 * np.ones_like(peak_time_activities[peak_time_activities[:, 1] != 1][:, 0]),\n",
    "         'v', ms=2)\n",
    "plt.xlim([0, 2000])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Photovoltage (V)')\n",
    "\n",
    "ax = ax.twinx()\n",
    "plt.plot(timestamps1, y, c='silver')\n",
    "plt.ylabel('Angles ($\\circ$)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 3000\n",
    "expanded_peaks = peak_expand((peak_time*1000).squeeze().astype(int) - 1, window_width, len(timestamps1))\n",
    "\n",
    "slices = []\n",
    "for idx in range(expanded_peaks.shape[0]):\n",
    "    slices.append(data1[expanded_peaks[idx, :]])\n",
    "slices = np.array(slices)\n",
    "\n",
    "slices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=1, nrows=4, figsize=(6, 8))\n",
    "\n",
    "for idx, slice in enumerate(slices):\n",
    "    if peak_activities[idx] == 1:\n",
    "        for ch, ax in enumerate(axes):\n",
    "            ax.plot(slice[:, ch], c='silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.__config__.show())\n",
    "print(f'PyTorch is running on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLHMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "class LHMDualDataset(BaseLHMDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        photovoltage_data = self.features[idx, :4, :].float().transpose(0, 1)  # First 4 channels\n",
    "        euler_angle_data = self.features[idx, 4:, :].float().transpose(0, 1)  # Last 3 channels\n",
    "        return photovoltage_data, euler_angle_data, self.labels[idx]\n",
    "    \n",
    "features = torch.tensor(slices).permute(0, 2, 1).type(torch.LongTensor)\n",
    "labels = peak_time_activities[:, 1]\n",
    "labels = torch.tensor(labels).type(torch.LongTensor)\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "dual_ds = LHMDualDataset(features, labels)\n",
    "dual_ds.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim1 = 4 # For photovoltage data\n",
    "input_dim2 = 3 # For Euler angle data\n",
    "hidden_dim = 256\n",
    "layer_dim = 1\n",
    "output_dim = len(np.unique(peak_activities.ravel())) # Number of classes\n",
    "dropout_prob = 0\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Preparing data\n",
    "batch_size = 32\n",
    "\n",
    "total_samples = len(dual_ds)\n",
    "test_size = int(0.2 * total_samples)\n",
    "train_size = total_samples - test_size\n",
    "\n",
    "train_dataset_dual, test_dataset_dual = random_split(dual_ds, [train_size, test_size])\n",
    "train_loader_dual = DataLoader(train_dataset_dual, batch_size=batch_size, shuffle=False)\n",
    "test_loader_dual = DataLoader(test_dataset_dual, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DualGRUModel(nn.Module):\n",
    "    def __init__(self, input_dim1, input_dim2, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru1 = nn.GRU(input_dim1, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "        self.gru2 = nn.GRU(input_dim2, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1, _ = self.gru1(x1)\n",
    "        out2, _ = self.gru2(x2)\n",
    "\n",
    "        # Concatenate the outputs from both GRUs\n",
    "        out = torch.cat((out1[:, -1, :], out2[:, -1, :]), 1)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DualGRUModel(input_dim1, input_dim2, hidden_dim, layer_dim, output_dim, dropout_prob)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container for losses and accuracies\n",
    "train_losses_gru_dual = []\n",
    "train_accuracies_gru_dual = []\n",
    "\n",
    "test_losses_gru_dual = []\n",
    "test_accuracies_gru_dual = []\n",
    "\n",
    "# Model definition\n",
    "model_gru_dual = DualGRUModel(input_dim1, input_dim2, hidden_dim, layer_dim, output_dim, dropout_prob)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_dual = torch.optim.Adam(model_gru_dual.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "print(model_gru_dual.to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_gru_dual.train()  # Set the model to training mode\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    temp_out_pack = torch.zeros((1, model_gru_dual.output_dim)).to(device)\n",
    "\n",
    "    for photovoltage_data, euler_angle_data, labels in train_loader_dual:\n",
    "        # Load data\n",
    "        photovoltage_data = photovoltage_data.to(device)\n",
    "        euler_angle_data = euler_angle_data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_gru_dual(photovoltage_data, euler_angle_data)\n",
    "        # print(temp_out_pack.shape)\n",
    "        # print(outputs.shape)\n",
    "        temp_out_pack = torch.vstack((temp_out_pack, outputs))\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer_dual.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_dual.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train_samples += labels.size(0)\n",
    "        total_train_correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader_dual)\n",
    "    train_accuracy = total_train_correct / total_train_samples\n",
    "    train_losses_gru_dual.append(avg_train_loss)\n",
    "    train_accuracies_gru_dual.append(train_accuracy)\n",
    "\n",
    "    model_gru_dual.eval()  # Set the model to evaluation mode\n",
    "    total_test_loss = 0.0\n",
    "    total_test_correct = 0\n",
    "    total_test_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for photovoltage_data, euler_angle_data, labels in test_loader_dual:\n",
    "            # Load data\n",
    "            photovoltage_data = photovoltage_data.to(device).float()\n",
    "            euler_angle_data = euler_angle_data.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_gru_dual(photovoltage_data, euler_angle_data)\n",
    "            temp_out_pack = torch.vstack((temp_out_pack, outputs))\n",
    "            loss = criterion(outputs, labels.squeeze())\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test_samples += labels.size(0)\n",
    "            total_test_correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader_dual)\n",
    "    test_accuracy = total_test_correct / total_test_samples\n",
    "    test_losses_gru_dual.append(avg_test_loss)\n",
    "    test_accuracies_gru_dual.append(test_accuracy)\n",
    "\n",
    "    # Print training and testing statistics for each epoch\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "                f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies_gru_dual)\n",
    "plt.plot(test_accuracies_gru_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses_gru_dual)\n",
    "plt.plot(test_losses_gru_dual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_out_pack = -temp_out_pack.cpu().numpy()\n",
    "len(temp_out_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest_amplitude = temp_out_pack[1:, 0]\n",
    "# cough_amplitude = temp_out_pack[1:, 1]\n",
    "# exercise_amplitude = temp_out_pack[1:, 2]\n",
    "\n",
    "x = np.arange(len(temp_out_pack) - 1)\n",
    "bottom = np.zeros_like(x).astype('float64')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for idx in range(3):\n",
    "    ax.bar(x, temp_out_pack[1:, idx], 1, bottom=bottom)\n",
    "    bottom += temp_out_pack[1:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(outputs, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
